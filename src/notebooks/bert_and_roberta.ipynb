{
 "cells": [
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "SiLDBLU36pBG",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1669151400536,
     "user_tz": -60,
     "elapsed": 368,
     "user": {
      "displayName": "Vadim Shishkin",
      "userId": "10839711751552203335"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 767
    },
    "executionInfo": {
     "elapsed": 27676,
     "status": "ok",
     "timestamp": 1669151428748,
     "user": {
      "displayName": "Vadim Shishkin",
      "userId": "10839711751552203335"
     },
     "user_tz": -60
    },
    "id": "B0iqx_LwbDA3",
    "outputId": "0fef0a5f-90da-4d28-e6e4-6e31d37101d3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: jax in /usr/local/lib/python3.7/dist-packages (0.3.25)\n",
      "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.7/dist-packages (from jax) (1.7.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jax) (4.1.1)\n",
      "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from jax) (1.21.6)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install jax\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "bert_checkpoint = 'bert-base-cased'\n",
    "roberta_checkpoint = 'roberta-base'\n",
    "\n",
    "checkpoint = bert_checkpoint\n",
    "\n",
    "datapath = \"bbc_train.csv\"\n",
    "df = pd.read_csv(datapath)\n",
    "df.groupby(['category']).size().plot.bar()\n",
    "tokenizer = BertTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "labels = {'business':0,\n",
    "          'sport':1,\n",
    "          'politics':2\n",
    "          # 'entertainment':3,\n",
    "          # 'tech':4,\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "df.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "UZKJQkKA87rt",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1669151428760,
     "user_tz": -60,
     "elapsed": 300,
     "user": {
      "displayName": "Vadim Shishkin",
      "userId": "10839711751552203335"
     }
    },
    "outputId": "4c2ea47d-2ccc-44ad-ae92-3d261eab2cb0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text  category\n",
       "0  Quarterly profits at US media giant TimeWarner...  business\n",
       "1  The dollar has hit its highest level against t...  business\n",
       "2  The owners of embattled Russian oil giant Yuko...  business\n",
       "3  British Airways has blamed high fuel prices fo...  business\n",
       "4  Shares in UK drinks and food firm Allied Domec...  business"
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-27663af0-4fbc-4b13-a726-2db6124a38d5\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Quarterly profits at US media giant TimeWarner...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The dollar has hit its highest level against t...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The owners of embattled Russian oil giant Yuko...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>British Airways has blamed high fuel prices fo...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shares in UK drinks and food firm Allied Domec...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-27663af0-4fbc-4b13-a726-2db6124a38d5')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-27663af0-4fbc-4b13-a726-2db6124a38d5 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-27663af0-4fbc-4b13-a726-2db6124a38d5');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 254,
     "status": "ok",
     "timestamp": 1669151428765,
     "user": {
      "displayName": "Vadim Shishkin",
      "userId": "10839711751552203335"
     },
     "user_tz": -60
    },
    "id": "_jQmrfsmqgfl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.labels = [labels[label] for label in df['category']]\n",
    "        self.texts = [tokenizer(text, \n",
    "                               padding='max_length', max_length = 512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['text']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install torchmetrics"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aFT7hscwN3_3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1669151434937,
     "user_tz": -60,
     "elapsed": 6424,
     "user": {
      "displayName": "Vadim Shishkin",
      "userId": "10839711751552203335"
     }
    },
    "outputId": "a1c775bc-d82f-41c8-d85d-a2dc88cf72ef",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (0.10.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
      "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.1+cu113)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizer, BertModel, AutoModelForSequenceClassification\n",
    "from torch import nn\n",
    "#import evaluate as metric_evaluate\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.functional import auc, precision_recall\n",
    "from torchmetrics.classification import MulticlassF1Score, MulticlassROC\n",
    "\n",
    "class TextClassifier(nn.Module): \n",
    "\n",
    "    def __init__(self, checkpoint, num_of_classes, dropout=0.5):\n",
    "\n",
    "        super(TextClassifier, self).__init__()\n",
    "\n",
    "        self.bert = AutoModelForSequenceClassification.from_pretrained(checkpoint,\n",
    "                                                                     num_labels=num_of_classes)\n",
    "        self.num_of_labels = num_of_classes\n",
    "\n",
    "    def forward(self, input_id, mask, labels):\n",
    "\n",
    "        pooled_output = self.bert(input_ids= input_id, attention_mask=mask,\n",
    "                                  labels=labels, return_dict=True)\n",
    "\n",
    "        return pooled_output\n",
    "\n",
    "def train(model, train_data, val_data, learning_rate, epochs, train_batch_size, val_batch_size):\n",
    "\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=train_batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=val_batch_size)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "            total_auc_train = 0\n",
    "            total_recall_train = 0\n",
    "            total_precision_train = 0\n",
    "            total_f1_train = 0\n",
    "            # total_fpr_train = 0\n",
    "            # total_tpr_train = 0\n",
    "            # total_thresholds_train = 0\n",
    "\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "                train_label = train_label.to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask, train_label)\n",
    "                #print(f\"\\n output:{output} \\n\")\n",
    "                \n",
    "                probabilities = F.softmax(output.logits, dim=1)\n",
    "                #print(f\"probabilities: {probabilities}\")\n",
    "\n",
    "                loss = criterion(probabilities, train_label.long())\n",
    "                #print(f\"loss: {loss}\")\n",
    "\n",
    "                (predictions, predictions_indicies) = torch.max(probabilities,dim=1)\n",
    "\n",
    "                # print(f\"predictions: {predictions}\")\n",
    "                # print(f\"train_label: {train_label}\")\n",
    "\n",
    "                metric_auc = auc(predictions, train_label, reorder=True)\n",
    "                # print(f\"metric_auc: {metric_auc}\")\n",
    "\n",
    "                metric_precision_recall = precision_recall(preds=probabilities, target=train_label,num_classes=model.num_of_labels)\n",
    "                precision, recall = metric_precision_recall[0], metric_precision_recall[1]\n",
    "                # print(f\"\\nprecision: {precision}\\n\")\n",
    "                # print(f\"recall: {recall}\\n\")\n",
    "\n",
    "                metric_f1 = MulticlassF1Score(num_classes=model.num_of_labels).to(device)\n",
    "                f1 = metric_f1(predictions, train_label)\n",
    "                #print(f\"f1: {f1}\\n\")\n",
    "\n",
    "                # metric_roc = MulticlassROC(num_classes=5)\n",
    "                # fpr_tpr_thresholds_tuple = metric_roc(probabilities, train_label)\n",
    "                # print(f\"fpr: {fpr_tpr_thresholds_tuple[0]}\\n\")\n",
    "                # print(f\"tpr: {fpr_tpr_thresholds_tuple[1]}\\n\")\n",
    "                # print(f\"thresholds: {fpr_tpr_thresholds_tuple[2]}\\n\")\n",
    "\n",
    "                total_loss_train += loss\n",
    "              \n",
    "                acc = (predictions_indicies == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                total_auc_train += metric_auc\n",
    "                total_recall_train += recall\n",
    "                total_precision_train += precision\n",
    "                total_f1_train += f1\n",
    "                # total_fpr_train += fpr_tpr_thresholds_tuple[0]\n",
    "                # total_tpr_train += fpr_tpr_thresholds_tuple[1]\n",
    "                #total_thresholds_train += fpr_tpr_thresholds_tuple[3]\n",
    "\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "            total_recall_val = 0\n",
    "            total_precision_val = 0\n",
    "            total_f1_val = 0\n",
    "            total_auc_val = 0\n",
    "            # total_fpr_val = 0\n",
    "            # total_tpr_val = 0\n",
    "            # total_thresholds_val = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_id, mask, val_label)\n",
    "                    #batch_loss = criterion(output, val_label.long())\n",
    "                    #total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    probabilities = F.softmax(output.logits, dim=1)\n",
    "\n",
    "                    loss = criterion(probabilities, val_label.long())\n",
    "                    #print(f\"loss: {loss}\")\n",
    "\n",
    "                    (predictions, predictions_indicies) = torch.max(probabilities,dim=1)\n",
    "\n",
    "                    # print(f\"predictions: {predictions}\")\n",
    "                    # print(f\"train_label: {train_label}\")\n",
    "\n",
    "                    metric_auc = auc(predictions, val_label, reorder=True)\n",
    "                    # print(f\"metric_auc: {metric_auc}\")\n",
    "\n",
    "                    metric_precision_recall = precision_recall(preds=probabilities, target=val_label,num_classes=model.num_of_labels)\n",
    "                    precision, recall = metric_precision_recall[0], metric_precision_recall[1]\n",
    "                    # print(f\"\\nprecision: {precision}\\n\")\n",
    "                    # print(f\"recall: {recall}\\n\")\n",
    "\n",
    "                    metric_f1 = MulticlassF1Score(num_classes=model.num_of_labels).to(device)\n",
    "                    f1 = metric_f1(predictions, val_label)\n",
    "                    #print(f\"f1: {f1}\\n\")\n",
    "\n",
    "                    # metric_roc = MulticlassROC(num_classes=5)\n",
    "                    # fpr_tpr_thresholds_tuple = metric_roc(probabilities, train_label)\n",
    "                    # print(f\"fpr: {fpr_tpr_thresholds_tuple[0]}\\n\")\n",
    "                    # print(f\"tpr: {fpr_tpr_thresholds_tuple[1]}\\n\")\n",
    "                    # print(f\"thresholds: {fpr_tpr_thresholds_tuple[2]}\\n\")\n",
    "\n",
    "                    total_loss_val += loss\n",
    "                  \n",
    "                    acc = (predictions_indicies == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "\n",
    "                    total_auc_val += metric_auc\n",
    "                    total_recall_val += recall\n",
    "                    total_precision_val += precision\n",
    "                    total_f1_val += f1\n",
    "\n",
    "            print(f\"Epochs: {epoch_num + 1} | Train Accuracy: {total_acc_train / len(train_data): .3f} | Train Loss: {total_loss_train / len(train_data): .3f} | Train Recall: {total_recall_train / len(train_data): .3f} | Train Precision: {total_precision_train / len(train_data): .3f} | Train F1-Score: {total_f1_train / len(train_data): .3f} | Train AUC: {total_auc_train/ len(train_data): .3f} | Val Accuracy: {total_acc_val / len(val_data): .3f} | Val Loss: {total_loss_val / len(val_data): .3f} | TraValin Recall: {total_recall_val / len(val_data): .3f} | Val Precision: {total_precision_val / len(val_data): .3f} | Val F1-Score: {total_f1_val / len(val_data): .3f} | Val AUC: {total_auc_val/ len(val_data): .3f}\")\n",
    "                  \n",
    "def evaluate(model, test_data, test_batch_size):\n",
    "\n",
    "    test = Dataset(test_data)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=test_batch_size)\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0\n",
    "    #total_loss_test = 0\n",
    "    total_auc_test = 0\n",
    "    total_recall_test = 0\n",
    "    total_precision_test = 0\n",
    "    total_f1_test = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_input, test_label in test_dataloader:\n",
    "\n",
    "              test_label = test_label.to(device)\n",
    "              mask = test_input['attention_mask'].to(device)\n",
    "              input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "              output = model(input_id, mask, test_label)\n",
    "              #batch_loss = criterion(output, val_label.long())\n",
    "              #total_loss_val += batch_loss.item()\n",
    "              \n",
    "              probabilities = F.softmax(output.logits, dim=1)\n",
    "\n",
    "              #loss = criterion(probabilities, test_label.long())\n",
    "              #print(f\"loss: {loss}\")\n",
    "\n",
    "              (predictions, predictions_indicies) = torch.max(probabilities,dim=1)\n",
    "\n",
    "              # print(f\"predictions: {predictions}\")\n",
    "              # print(f\"train_label: {train_label}\")\n",
    "\n",
    "              metric_auc = auc(predictions, test_label, reorder=True)\n",
    "              # print(f\"metric_auc: {metric_auc}\")\n",
    "\n",
    "              metric_precision_recall = precision_recall(preds=probabilities, target=test_label,num_classes=model.num_of_labels)\n",
    "              precision, recall = metric_precision_recall[0], metric_precision_recall[1]\n",
    "              # print(f\"\\nprecision: {precision}\\n\")\n",
    "              # print(f\"recall: {recall}\\n\")\n",
    "\n",
    "              metric_f1 = MulticlassF1Score(num_classes=model.num_of_labels).to(device)\n",
    "              f1 = metric_f1(predictions, test_label)\n",
    "              #print(f\"f1: {f1}\\n\")\n",
    "\n",
    "              # metric_roc = MulticlassROC(num_classes=5)\n",
    "              # fpr_tpr_thresholds_tuple = metric_roc(probabilities, train_label)\n",
    "              # print(f\"fpr: {fpr_tpr_thresholds_tuple[0]}\\n\")\n",
    "              # print(f\"tpr: {fpr_tpr_thresholds_tuple[1]}\\n\")\n",
    "              # print(f\"thresholds: {fpr_tpr_thresholds_tuple[2]}\\n\")\n",
    "\n",
    "              #total_loss_val += loss\n",
    "\n",
    "              acc = (predictions_indicies == test_label).sum().item()\n",
    "              total_acc_test += acc\n",
    "\n",
    "              total_auc_test += metric_auc\n",
    "              total_recall_test += recall\n",
    "              total_precision_test += precision\n",
    "              total_f1_test += f1\n",
    "  \n",
    "    print(f\"Test Accuracy: {total_acc_test / len(test_data): .3f} | Test Recall: {total_recall_test / len(test_data): .3f} | Test Precision: {total_precision_test / len(test_data): .3f} | Test F1-Score: {total_f1_test / len(test_data): .3f} | Test AUC: {total_auc_test/ len(test_data): .3f}\")\n",
    "\n"
   ],
   "metadata": {
    "id": "7pecW3opo6k3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1669151435960,
     "user_tz": -60,
     "elapsed": 1050,
     "user": {
      "displayName": "Vadim Shishkin",
      "userId": "10839711751552203335"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "np.random.seed(42)\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), \n",
    "                                     [int(.8*len(df)), int(.9*len(df))])\n",
    "\n",
    "print(len(df_train),len(df_val), len(df_test))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jVJrQ_uJKfVc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1669151435963,
     "user_tz": -60,
     "elapsed": 96,
     "user": {
      "displayName": "Vadim Shishkin",
      "userId": "10839711751552203335"
     }
    },
    "outputId": "248a27eb-d4ae-427a-8752-d05afe9442d8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1150 144 144\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "num_of_classes=3\n",
    "model = TextClassifier(checkpoint, num_of_classes=num_of_classes)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "quVJK_YPKheS",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1669151438904,
     "user_tz": -60,
     "elapsed": 3033,
     "user": {
      "displayName": "Vadim Shishkin",
      "userId": "10839711751552203335"
     }
    },
    "outputId": "f6d00e5c-fc91-43f2-a5f5-7464963b99cc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "EPOCHS = 1\n",
    "LR = 1e-6\n",
    "train_batch_size = 16\n",
    "val_batch_size = 12\n",
    "\n",
    "train(model, df_train, df_val, LR, EPOCHS, train_batch_size, val_batch_size)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "3fE2LX_IGhVb",
    "executionInfo": {
     "status": "error",
     "timestamp": 1669151610032,
     "user_tz": -60,
     "elapsed": 171140,
     "user": {
      "displayName": "Vadim Shishkin",
      "userId": "10839711751552203335"
     }
    },
    "outputId": "a9883c9a-cd32-4b58-8cb3-11515e98cb10",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  1%|▏         | 1/72 [02:21<2:47:19, 141.40s/it]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-29c9cfac41e6>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mval_batch_size\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m12\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdf_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdf_val\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mLR\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mEPOCHS\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_batch_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_batch_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-5-9ac5f07a1e1f>\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(model, train_data, val_data, learning_rate, epochs, train_batch_size, val_batch_size)\u001B[0m\n\u001B[1;32m     59\u001B[0m                 \u001B[0minput_id\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_input\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'input_ids'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msqueeze\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     60\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 61\u001B[0;31m                 \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_label\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     62\u001B[0m                 \u001B[0;31m#print(f\"\\n output:{output} \\n\")\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1131\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-5-9ac5f07a1e1f>\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input_id, mask, labels)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m         pooled_output = self.bert(input_ids= input_id, attention_mask=mask,\n\u001B[0;32m---> 21\u001B[0;31m                                   labels=labels, return_dict=True)\n\u001B[0m\u001B[1;32m     22\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mpooled_output\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1131\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1559\u001B[0m             \u001B[0moutput_attentions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_attentions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1560\u001B[0m             \u001B[0moutput_hidden_states\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_hidden_states\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1561\u001B[0;31m             \u001B[0mreturn_dict\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mreturn_dict\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1562\u001B[0m         )\n\u001B[1;32m   1563\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1131\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1022\u001B[0m             \u001B[0moutput_attentions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_attentions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1023\u001B[0m             \u001B[0moutput_hidden_states\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_hidden_states\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1024\u001B[0;31m             \u001B[0mreturn_dict\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mreturn_dict\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1025\u001B[0m         )\n\u001B[1;32m   1026\u001B[0m         \u001B[0msequence_output\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mencoder_outputs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1131\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    608\u001B[0m                     \u001B[0mencoder_attention_mask\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    609\u001B[0m                     \u001B[0mpast_key_value\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 610\u001B[0;31m                     \u001B[0moutput_attentions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    611\u001B[0m                 )\n\u001B[1;32m    612\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1131\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    530\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    531\u001B[0m         layer_output = apply_chunking_to_forward(\n\u001B[0;32m--> 532\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeed_forward_chunk\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mchunk_size_feed_forward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mseq_len_dim\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattention_output\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    533\u001B[0m         )\n\u001B[1;32m    534\u001B[0m         \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mlayer_output\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0moutputs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/transformers/pytorch_utils.py\u001B[0m in \u001B[0;36mapply_chunking_to_forward\u001B[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001B[0m\n\u001B[1;32m    244\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput_chunks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mchunk_dim\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    245\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 246\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mforward_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput_tensors\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    247\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    248\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001B[0m in \u001B[0;36mfeed_forward_chunk\u001B[0;34m(self, attention_output)\u001B[0m\n\u001B[1;32m    541\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    542\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mfeed_forward_chunk\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattention_output\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 543\u001B[0;31m         \u001B[0mintermediate_output\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mintermediate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mattention_output\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    544\u001B[0m         \u001B[0mlayer_output\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moutput\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mintermediate_output\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattention_output\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    545\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlayer_output\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1131\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, hidden_states)\u001B[0m\n\u001B[1;32m    441\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    442\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhidden_states\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 443\u001B[0;31m         \u001B[0mhidden_states\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdense\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhidden_states\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    444\u001B[0m         \u001B[0mhidden_states\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mintermediate_act_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhidden_states\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    445\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mhidden_states\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1131\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    113\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 114\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlinear\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    115\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mextra_repr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "test_batch_size = 12\n",
    "evaluate(model, df_test, test_batch_size)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "EKIL-imZFrff",
    "executionInfo": {
     "status": "error",
     "timestamp": 1669151648053,
     "user_tz": -60,
     "elapsed": 35510,
     "user": {
      "displayName": "Vadim Shishkin",
      "userId": "10839711751552203335"
     }
    },
    "outputId": "5bfb5d2c-7698-473c-8eee-99eb2bbacc4f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-9-c50daf07a509>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mtest_batch_size\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m12\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mevaluate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdf_test\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_batch_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-5-9ac5f07a1e1f>\u001B[0m in \u001B[0;36mevaluate\u001B[0;34m(model, test_data, test_batch_size)\u001B[0m\n\u001B[1;32m    197\u001B[0m               \u001B[0minput_id\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtest_input\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'input_ids'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msqueeze\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    198\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 199\u001B[0;31m               \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_label\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    200\u001B[0m               \u001B[0;31m#batch_loss = criterion(output, val_label.long())\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m               \u001B[0;31m#total_loss_val += batch_loss.item()\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1131\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-5-9ac5f07a1e1f>\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input_id, mask, labels)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m         pooled_output = self.bert(input_ids= input_id, attention_mask=mask,\n\u001B[0;32m---> 21\u001B[0;31m                                   labels=labels, return_dict=True)\n\u001B[0m\u001B[1;32m     22\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mpooled_output\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1131\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1559\u001B[0m             \u001B[0moutput_attentions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_attentions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1560\u001B[0m             \u001B[0moutput_hidden_states\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_hidden_states\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1561\u001B[0;31m             \u001B[0mreturn_dict\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mreturn_dict\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1562\u001B[0m         )\n\u001B[1;32m   1563\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1131\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1022\u001B[0m             \u001B[0moutput_attentions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_attentions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1023\u001B[0m             \u001B[0moutput_hidden_states\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_hidden_states\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1024\u001B[0;31m             \u001B[0mreturn_dict\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mreturn_dict\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1025\u001B[0m         )\n\u001B[1;32m   1026\u001B[0m         \u001B[0msequence_output\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mencoder_outputs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1131\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    608\u001B[0m                     \u001B[0mencoder_attention_mask\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    609\u001B[0m                     \u001B[0mpast_key_value\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 610\u001B[0;31m                     \u001B[0moutput_attentions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    611\u001B[0m                 )\n\u001B[1;32m    612\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1131\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    530\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    531\u001B[0m         layer_output = apply_chunking_to_forward(\n\u001B[0;32m--> 532\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeed_forward_chunk\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mchunk_size_feed_forward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mseq_len_dim\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattention_output\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    533\u001B[0m         )\n\u001B[1;32m    534\u001B[0m         \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mlayer_output\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0moutputs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/transformers/pytorch_utils.py\u001B[0m in \u001B[0;36mapply_chunking_to_forward\u001B[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001B[0m\n\u001B[1;32m    244\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput_chunks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mchunk_dim\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    245\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 246\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mforward_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput_tensors\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    247\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    248\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001B[0m in \u001B[0;36mfeed_forward_chunk\u001B[0;34m(self, attention_output)\u001B[0m\n\u001B[1;32m    542\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mfeed_forward_chunk\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattention_output\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    543\u001B[0m         \u001B[0mintermediate_output\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mintermediate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mattention_output\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 544\u001B[0;31m         \u001B[0mlayer_output\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moutput\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mintermediate_output\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattention_output\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    545\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlayer_output\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    546\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1131\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, hidden_states, input_tensor)\u001B[0m\n\u001B[1;32m    454\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    455\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhidden_states\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_tensor\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 456\u001B[0;31m         \u001B[0mhidden_states\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdense\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhidden_states\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    457\u001B[0m         \u001B[0mhidden_states\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdropout\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhidden_states\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    458\u001B[0m         \u001B[0mhidden_states\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mLayerNorm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhidden_states\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0minput_tensor\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1131\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    113\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 114\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlinear\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    115\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mextra_repr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "r2_cb0r56f8X",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1669151610036,
     "user_tz": -60,
     "elapsed": 16,
     "user": {
      "displayName": "Vadim Shishkin",
      "userId": "10839711751552203335"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "gYigMMKz8q9z",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1669151610038,
     "user_tz": -60,
     "elapsed": 17,
     "user": {
      "displayName": "Vadim Shishkin",
      "userId": "10839711751552203335"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyPBDzbOidSD7LIprvCb//0U"
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}